{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HARDWARE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "\"\"\"\n",
    "To change attributes:\n",
    "1) Change ATTRIBUTE and you're good to go\n",
    "\"\"\"\n",
    "ATTRIBUTE = 'polarity'\n",
    "COUNTER = '_new'\n",
    "PARALLEL = 1\n",
    "PARALLEL_F = 1\n",
    "PARALLEL_EXTRACTION = 1\n",
    "TRAIN_SIZE = 5\n",
    "DEV_SIZE = 5\n",
    "TEST_SIZE = 5\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['SNORKELDBNAME'] = ATTRIBUTE + str(COUNTER)\n",
    "os.environ['SNORKELDB'] = 'postgres://localhost:5432'# + os.environ['SNORKELDBNAME']\n",
    "        \n",
    "sys.path.append(os.environ['SNORKELHOME'] + '/tutorials/fonduer/tables/')\n",
    "snorkel_postgres = os.environ['SNORKELDB'].startswith('postgres')\n",
    "print snorkel_postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "SNORKELDB = postgres://localhost:5432\n",
      "SNORKELDBNAME = polarity_new\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if snorkel_postgres:\n",
    "#     os.environ['SNORKELDBNAME'] = ATTRIBUTE + str(COUNTER) +'2'\n",
    "    print os.system(\"dropdb \" + os.environ['SNORKELDBNAME'])\n",
    "    print os.system(\"createdb \" + os.environ['SNORKELDBNAME'])\n",
    "    print \"SNORKELDB = %s\" % os.environ['SNORKELDB']\n",
    "    print \"SNORKELDBNAME = %s\" % os.environ['SNORKELDBNAME']\n",
    "else:\n",
    "    try:\n",
    "        os.remove('snorkel.db')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "from fonduer import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "import os\n",
    "if snorkel_postgres:\n",
    "    from fonduer.async_parser import HTMLParser, AsyncOmniParser\n",
    "    from fonduer.models import Document, Sentence\n",
    "    print \"Starting async parse...\"\n",
    "    \n",
    "    doc_preprocessor = HTMLParser()\n",
    "    corpus_parser = AsyncOmniParser(blacklist=['style'], flatten=['span','br'], \n",
    "                                     tabular=True, lingual=True,\n",
    "                                     visual=True)\n",
    "    # PARSE TRAIN\n",
    "    corpus_name = 'Hardware Train'\n",
    "    docs_path = os.environ['SNORKELHOME'] + '/tutorials/fonduer/tables/data/hardware/train_digikey/html/'\n",
    "    pdf_path = os.environ['SNORKELHOME'] + '/tutorials/fonduer/tables/data/hardware/train_digikey/pdf/'\n",
    "\n",
    "    %time corpus_parser.apply(doc_preprocessor, docs_path, pdf_path, session, \\\n",
    "                                max_docs=TRAIN_SIZE, parallel=PARALLEL)\n",
    "    session.commit()\n",
    "    print \"Documents:\", session.query(Document).count()\n",
    "    print \"Sentences:\", session.query(Sentence).count()\n",
    "# #     print \"%s contains %d documents\" % (corpus, len(corpus))\n",
    "#     session.commit()\n",
    "#     # PARSE DEV\n",
    "#     corpus_name = 'Hardware Dev'\n",
    "#     docs_path = os.environ['SNORKELHOME'] + '/tutorials/fonduer/tables/data/hardware/dev/html/'\n",
    "#     pdf_path = os.environ['SNORKELHOME'] + '/tutorials/fonduer/tables/data/hardware/dev/pdf/'\n",
    "    \n",
    "#     %time corpus_parser.apply(doc_preprocessor, docs_path, pdf_path, session, \\\n",
    "#                                 max_docs=DEV_SIZE, parallel=PARALLEL)\n",
    "# #     print \"%s contains %d documents\" % (corpus, len(corpus))\n",
    "#     session.commit()\n",
    "#     if TEST_SIZE:\n",
    "#         # PARSE TEST\n",
    "#         corpus_name = 'Hardware Test'\n",
    "#         docs_path = os.environ['SNORKELHOME'] + '/tutorials/fonduer/tables/data/hardware/test/html/'\n",
    "#         pdf_path = os.environ['SNORKELHOME'] + '/tutorials/fonduer/tables/data/hardware/test/pdf/'\n",
    "        \n",
    "#         %time corpus_parser.apply(doc_preprocessor, docs_path, pdf_path, session, \\\n",
    "#                                     max_docs=TEST_SIZE, parallel=PARALLEL)\n",
    "# #         print \"%s contains %d documents\" % (corpus, len(corpus))\n",
    "#         session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting async parse...\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "Documents: 10\n",
      "Phrases: 7113\n"
     ]
    }
   ],
   "source": [
    "# from fonduer.async_parser import HTMLParser, AsyncOmniParser\n",
    "from fonduer.parser import HTMLPreprocessor, OmniParser\n",
    "from fonduer.models import Document, Phrase\n",
    "print \"Starting async parse...\"\n",
    "\n",
    "\n",
    "# docs_path = os.environ['SNORKELHOME'] + '/tutorials/fonduer/tables/data/hardware/train_digikey/html/'\n",
    "# pdf_path = os.environ['SNORKELHOME'] + '/tutorials/fonduer/tables/data/hardware/train_digikey/pdf/'\n",
    "docs_path = os.environ['SNORKELHOME'] + '/tutorials/fonduer/tables/data/hardware/dev/html/'\n",
    "pdf_path = os.environ['SNORKELHOME'] + '/tutorials/fonduer/tables/data/hardware/dev/pdf/'\n",
    "\n",
    "doc_preprocessor = HTMLPreprocessor(docs_path, max_docs=10)\n",
    "\n",
    "# corpus_parser = OmniParser(blacklist=['style'], flatten=['span','br'], tabular=True, lingual=True, visual=True, pdf_path=pdf_path)\n",
    "corpus_parser = OmniParser(structural=True, lingual=True, visual=True, blacklist=['style'], flatten=['span','br'], pdf_path=pdf_path)\n",
    "corpus_parser.apply(doc_preprocessor, parallelism=4)\n",
    "\n",
    "# corpus_parser = AsyncOmniParser(blacklist=['style'], flatten=['span','br'], \n",
    "#                                  tabular=True, lingual=True,\n",
    "#                                  visual=True)\n",
    "\n",
    "# %time corpus_parser.apply(doc_preprocessor, docs_path, pdf_path, session, \\\n",
    "#                             max_docs=TRAIN_SIZE, parallel=PARALLEL)\n",
    "# session.commit()\n",
    "print \"Documents:\", session.query(Document).count()\n",
    "print \"Phrases:\", session.query(Phrase).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = session.query(Document).order_by(Document.name).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 820 9\n",
      "1 425 8\n",
      "2 1864 2\n",
      "3 673 14\n",
      "4 111 2\n",
      "5 332 6\n",
      "6 1750 88\n",
      "7 261 1\n",
      "8 369 6\n",
      "9 508 5\n"
     ]
    }
   ],
   "source": [
    "for i, d in enumerate(docs):\n",
    "    print i, len(d.phrases), len(d.tables)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0 188 5\n",
    "1 198 4\n",
    "2 159 4\n",
    "3 213 4\n",
    "4 190 5\n",
    "5 193 5\n",
    "6 198 4\n",
    "7 195 4\n",
    "8 178 4\n",
    "9 202 4\n",
    "10 220 4\n",
    "11 185 5\n",
    "12 241 4\n",
    "13 209 4\n",
    "14 294 6\n",
    "15 295 6\n",
    "16 295 6\n",
    "17 297 6\n",
    "18 228 5\n",
    "19 302 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features [168] [179]\n",
      "BC817, BC818 - NPN Epitaxial Silicon Transistor [61, 61, 61, 61, 61, 61, 61, 61] [341, 341, 341, 341, 341, 341, 341, 341]\n",
      "www.centralsemi.com [636] [644]\n",
      "See [486] [494]\n",
      "BASE [187] [196]\n",
      "BC546/7/8 [32] [49]\n",
      "- [170] [178]\n",
      "0.1 [301] [307]\n",
      "BC547C [379] [387]\n",
      "Zetex - BC860C Silicon planar general purpose transistor datasheet [27, 59, 52, 27, 27, 27, 44, 44, 29] [43, 70, 65, 43, 43, 43, 60, 60, 42]\n"
     ]
    }
   ],
   "source": [
    "for i, d in enumerate(docs):\n",
    "    print d.phrases[0].text, d.phrases[0].top, d.phrases[0].bottom"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2N3715 [522] [529]\n",
    "NPN High Power Silicon Transistor 2N5152 & 2N5154 Series 4-15-14_MMIC-Microstrip Pin Diodes.qxd [93, 93, 93, 93, 93, 116, 116, 116, 144, 93, 166, 93] [113, 113, 113, 113, 113, 129, 129, 129, 157, 113, 175, 113]\n",
    "NPN Power Silicon Transistor 2N5339 Series [51, 51, 51, 51, 73, 121] [70, 70, 70, 70, 86, 133]\n",
    "NPN High Power Silicon Transistor 2N6283 & 2N6284 Series [63, 63, 63, 86, 86, 114, 114, 114, 86] [83, 83, 83, 106, 106, 127, 127, 127, 106]\n",
    "metelics-sales@aeroflex.com [661] [676]\n",
    "VEBO [289] [300]\n",
    "Maximum Ratings [230, 230] [243, 243]\n",
    "Vdc [258] [266]\n",
    "Lawrence, MA 01840 [531, 531, 531, 531] [539, 539, 539, 539]\n",
    "Features [144] [157]\n",
    "Vdc [205] [213]\n",
    "PNP High Power Silicon Transistor 2N5679 & 2N5680 Series [72, 152, 72, 72, 72, 93, 93, 93, 164] [90, 161, 90, 90, 90, 106, 106, 106, 173]\n",
    "Features [98] [111]\n",
    "NPN High Power Silicon Transistor 2N6676 & 2N6678 Series [55, 55, 55, 55, 55, 78, 78, 78, 98] [75, 75, 75, 75, 75, 91, 91, 91, 111]\n",
    "0 [687] [694]\n",
    "BBD240.fm [172] [186]\n",
    "TIS631AA [490] [496]\n",
    "BBD242.fm [172] [186]\n",
    "70 [519] [526]\n",
    "BBD244.fm [172] [186]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0 188 5\n",
    "1 198 4\n",
    "2 159 4\n",
    "3 213 4\n",
    "4 190 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7113L"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fonduer.models import Document, Phrase\n",
    "\n",
    "\n",
    "session.query(Phrase).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fonduer.models import candidate_subclass\n",
    "\n",
    "Part_Attr = candidate_subclass('Part_Attr', ['part','attr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Matchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using combined matcher.\n"
     ]
    }
   ],
   "source": [
    "from hardware_matchers import get_matcher\n",
    "\n",
    "dict_path = os.environ['SNORKELHOME'] +\\\n",
    "    '/tutorials/fonduer/tables/data/hardware/gold_raw/digikey_part_dictionary.csv'\n",
    "part_matcher = get_matcher('part', dict_path)\n",
    "attr_matcher = get_matcher(ATTRIBUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ContextSpaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hardware_spaces import get_space\n",
    "    \n",
    "part_ngrams = get_space('part')\n",
    "attr_ngrams = get_space(ATTRIBUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Candidate Throttler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function polarity_throttler at 0x7f281489a668>\n"
     ]
    }
   ],
   "source": [
    "from hardware_throttlers import get_throttler\n",
    "\n",
    "throttler = get_throttler(ATTRIBUTE)\n",
    "# throttler = None\n",
    "print throttler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CandidateExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "CPU times: user 41.4 s, sys: 408 ms, total: 41.8 s\n",
      "Wall time: 49.3 s\n"
     ]
    }
   ],
   "source": [
    "from fonduer.candidates import CandidateExtractor\n",
    "# from snorkel.utils import get_ORM_instance\n",
    "# from fonduer.async_candidates import parallel_extract\n",
    "\n",
    "candidate_extractor = CandidateExtractor(Part_Attr, \n",
    "                        [part_ngrams, attr_ngrams], \n",
    "                        [part_matcher, attr_matcher], \n",
    "                        throttler=throttler)\n",
    "\n",
    "%time candidate_extractor.apply(docs, split=0)\n",
    "\n",
    "# corpus_names = ['Hardware Train', 'Hardware Dev']\n",
    "# if TEST_SIZE:\n",
    "#     corpus_names.append('Hardware Test')\n",
    "# for corpus_name in corpus_names:\n",
    "#     corpus = get_ORM_instance(Corpus, session, corpus_name)\n",
    "#     print \"Extracting Candidates from %s\" % corpus\n",
    "#     %time candidates = parallel_extract(session, candidate_extractor, corpus, \\\n",
    "#                                         corpus_name + ' Candidates', \\\n",
    "#                                         parallel=PARALLEL_EXTRACTION)\n",
    "#     session.add(candidates)\n",
    "#     print \"%s contains %d Candidates\" % (candidates, len(candidates))\n",
    "# session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 4536\n"
     ]
    }
   ],
   "source": [
    "train_cands = session.query(Part_Attr).filter(Part_Attr.split == 0).all()\n",
    "print \"Number of candidates:\", len(train_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part_Attr(Span(\"BC547ARL\", sentence=2807, chars=[0,7], words=[0,0]), Span(\"NPN\", sentence=122, chars=[0,2], words=[0,0]))\n",
      "BC547ARL\n",
      "NPN Silicon\n"
     ]
    }
   ],
   "source": [
    "print train_cands[0]\n",
    "print train_cands[0][0].sentence.text\n",
    "print train_cands[0][1].sentence.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from hardware_utils import get_gold_parts_by_doc, get_manual_parts_by_doc\n",
    "# from snorkel.utils import get_ORM_instance\n",
    "# from snorkel.models import Corpus\n",
    "\n",
    "# corpus = get_ORM_instance(Corpus, session, 'Hardware Dev')\n",
    "\n",
    "# # parts_by_doc = get_gold_parts_by_doc()\n",
    "# parts_by_doc = get_manual_parts_by_doc(corpus.documents.all())\n",
    "# # parts_by_doc = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import cPickle as pickle\n",
    "# pickle_file = os.environ['SNORKELHOME'] + '/tutorials/tables/sandbox/parts_by_doc_dev.pkl'\n",
    "\n",
    "# with open(pickle_file, 'w') as f:\n",
    "#     pickle.dump(parts_by_doc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "pickle_file = os.environ['SNORKELHOME'] + '/tutorials/fonduer/tables/sandbox/parts_by_doc_dev.pkl'\n",
    "with open(pickle_file, 'r') as f:\n",
    "    parts_by_doc = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bc546-d\n",
      "BC818\n",
      "CentralSemiconductorCorp_2N4013\n",
      "DiodesIncorporated_FZT651TC\n",
      "JCSTS01155-1\n",
      "KECCS05435-1\n",
      "PHGLS20125-1\n",
      "PNJIS01593-1\n",
      "SGSTS13702-1\n",
      "ZETXS01948-1\n"
     ]
    }
   ],
   "source": [
    "for i, d in enumerate(docs):\n",
    "    print d.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing candidates...\n",
      "[========================================] 100%\n",
      "========================================\n",
      "Scoring on Entity-Level Gold Data\n",
      "========================================\n",
      "Corpus Precision 0.801\n",
      "Corpus Recall    0.993\n",
      "Corpus F1        0.887\n",
      "----------------------------------------\n",
      "TP: 141 | FP: 35 | FN: 1\n",
      "========================================\n",
      "\n",
      "CPU times: user 1.48 s, sys: 76 ms, total: 1.56 s\n",
      "Wall time: 1.69 s\n"
     ]
    }
   ],
   "source": [
    "# from fonduer.models import Corpus, CandidateSet\n",
    "from hardware_utils import entity_level_f1\n",
    "\n",
    "# corpus = get_ORM_instance(Corpus, session, 'Hardware Dev')\n",
    "# candidates = get_ORM_instance(CandidateSet, session, 'Hardware Dev Candidates')\n",
    "candidates = train_cands\n",
    "\n",
    "gold_file = os.environ['SNORKELHOME'] + \\\n",
    "    '/tutorials/fonduer/tables/data/hardware/dev/hardware_dev_gold.csv'\n",
    "%time (ctp, cfp, cfn) = entity_level_f1(candidates, gold_file, ATTRIBUTE, docs, parts_by_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If necessary\n",
    "# import os\n",
    "# os.remove('snorkel.db');\n",
    "# os.system('cp snorkel.db\\ candidates snorkel.db');\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(os.environ['SNORKELHOME'] + '/tutorials/tables/')\n",
    "\n",
    "# from snorkel import SnorkelSession\n",
    "# session = SnorkelSession()\n",
    "\n",
    "# from snorkel.models import candidate_subclass\n",
    "# Part_Attr = candidate_subclass('Part_Attr', ['part','attr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from fonduer.models import CandidateSet\n",
    "# from hardware_utils import load_hardware_labels\n",
    "\n",
    "# data_sets = ['Dev']\n",
    "# gold_file = {}\n",
    "# gold_file['Dev'] = os.environ['SNORKELHOME'] + '/tutorials/fonduer/tables/data/hardware/dev/hardware_dev_gold.csv'\n",
    "# if TEST_SIZE:\n",
    "#     data_sets.append('Test')\n",
    "#     gold_file['Test'] = os.environ['SNORKELHOME'] + '/tutorials/fonduer/tables/data/hardware/test/hardware_test_gold.csv'\n",
    "# for data_set in data_sets:\n",
    "#     candidate_set_name = 'Hardware %s Candidates' % data_set\n",
    "#     candidates = session.query(CandidateSet).filter(\n",
    "#         CandidateSet.name == candidate_set_name).one()\n",
    "#     label_set_name = 'Hardware %s Candidates -- Gold' % data_set\n",
    "#     annotation_key_name = 'Hardware %s Labels -- Gold' % data_set\n",
    "#     %time gold_candidates, annotation_key = load_hardware_labels(session,\\\n",
    "#                            label_set_name, \\\n",
    "#                            annotation_key_name, \\\n",
    "#                            candidates, \\\n",
    "#                            gold_file[data_set], \\\n",
    "#                            ATTRIBUTE)\n",
    "#     candidates_gold = session.query(CandidateSet).filter(\n",
    "#         CandidateSet.name == candidate_set_name + ' -- Gold').one()\n",
    "#     print \"%d/%d Candidates in %s have positive Labels\" % (\n",
    "#         len(candidates_gold), len(candidates), candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('CORE_e1_SPAN_TYPE_[EXPLICIT]', 1)\n",
      "('CORE_e1_STARTS_WITH_CAPITAL', 1)\n",
      "('CORE_e1_LENGTH_1', 1)\n",
      "('CORE_e2_SPAN_TYPE_[EXPLICIT]', 1)\n",
      "('CORE_e2_STARTS_WITH_CAPITAL', 1)\n",
      "('CORE_e2_LENGTH_1', 1)\n",
      "(u'DDL_e1_WORD_SEQ_[BC547ARL]', 1)\n",
      "(u'DDL_e1_LEMMA_SEQ_[bc547arl]', 1)\n",
      "(u'DDL_e1_POS_SEQ_[NN]', 1)\n",
      "(u'DDL_e1_DEP_SEQ_[ROOT]', 1)\n",
      "(u'DDL_e1_W_LEFT_1_[bc547arl]', 1)\n",
      "(u'DDL_e1_W_LEFT_POS_1_[NN]', 1)\n",
      "(u'DDL_e2_WORD_SEQ_[NPN]', 1)\n",
      "(u'DDL_e2_LEMMA_SEQ_[NPN]', 1)\n",
      "(u'DDL_e2_POS_SEQ_[NNP]', 1)\n",
      "(u'DDL_e2_DEP_SEQ_[compound]', 1)\n",
      "(u'DDL_e2_W_LEFT_1_[Silicon]', 1)\n",
      "(u'DDL_e2_W_LEFT_POS_1_[NNP]', 1)\n",
      "(u'DDL_e2_W_LEFT_2_[NPN Silicon]', 1)\n",
      "(u'DDL_e2_W_LEFT_POS_2_[NNP NNP]', 1)\n",
      "(u'DDL_e2_W_RIGHT_1_[Silicon]', 1)\n",
      "(u'DDL_e2_W_RIGHT_POS_1_[NNP]', 1)\n",
      "(u'DDL_e2_W_LEMMA_L_1_R_1_[Silicon]_[Silicon]', 1)\n",
      "(u'DDL_e2_W_POS_L_1_R_1_[NNP]_[NNP]', 1)\n",
      "(u'DDL_e2_W_LEMMA_L_2_R_1_[NPN Silicon]_[Silicon]', 1)\n",
      "(u'DDL_e2_W_POS_L_2_R_1_[NNP NNP]_[NNP]', 1)\n",
      "('STR_e1_TAG_p', 1)\n",
      "(u'STR_e1_HTML_ATTR_class=s5', 1)\n",
      "(u'STR_e1_HTML_ATTR_style=padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: left;', 1)\n",
      "('STR_e1_PARENT_TAG_td', 1)\n",
      "('STR_e1_FIRST_NODE', 1)\n",
      "('STR_e1_LAST_NODE', 1)\n",
      "('STR_e1_ANCESTOR_CLASS_[None None None None None s5]', 1)\n",
      "('STR_e1_ANCESTOR_TAG_[html body table tr td p]', 1)\n",
      "('STR_e1_ANCESTOR_ID_[None None None None None None]', 1)\n",
      "('STR_e2_TAG_h2', 1)\n",
      "(u'STR_e2_HTML_ATTR_style=padding-top: 5pt;padding-left: 6pt;text-indent: 0pt;text-align: left;', 1)\n",
      "('STR_e2_PARENT_TAG_body', 1)\n",
      "('STR_e2_PREV_SIB_TAG_h1', 1)\n",
      "('STR_e2_NODE_POS_5', 1)\n",
      "('STR_e2_NEXT_SIB_TAG_p', 1)\n",
      "('STR_e2_ANCESTOR_CLASS_[None None None]', 1)\n",
      "('STR_e2_ANCESTOR_TAG_[html body h2]', 1)\n",
      "('STR_e2_ANCESTOR_ID_[None None None]', 1)\n",
      "(u'STR_COMMON_ANCESTOR_[ html body]', 1)\n",
      "('STR_LOWEST_ANCESTOR_DEPTH_[1]', 1)\n",
      "('TAB_e1_ROW_NUM_[6]', 1)\n",
      "('TAB_e1_COL_NUM_[0]', 1)\n",
      "('TAB_e1_ROW_SPAN_[1]', 1)\n",
      "('TAB_e1_COL_SPAN_[1]', 1)\n",
      "(u'TAB_e1_COL_HEAD_WORDS_[device]', 1)\n",
      "(u'TAB_e1_ROW_WORDS_[to-92]', 1)\n",
      "(u'TAB_e1_ROW_WORDS_[2000]', 1)\n",
      "(u'TAB_e1_ROW_WORDS_[2000 /]', 1)\n",
      "(u'TAB_e1_ROW_WORDS_[/]', 1)\n",
      "(u'TAB_e1_ROW_WORDS_[/ tape]', 1)\n",
      "(u'TAB_e1_ROW_WORDS_[tape]', 1)\n",
      "(u'TAB_e1_ROW_WORDS_[tape &]', 1)\n",
      "(u'TAB_e1_ROW_WORDS_[&]', 1)\n",
      "(u'TAB_e1_ROW_WORDS_[& reel]', 1)\n",
      "(u'TAB_e1_ROW_WORDS_[reel]', 1)\n",
      "(u'TAB_e1_COL_WORDS_[device]', 1)\n",
      "(u'TAB_e1_COL_WORDS_[bc546b]', 1)\n",
      "(u'TAB_e1_COL_WORDS_[bc546bg]', 1)\n",
      "(u'TAB_e1_COL_WORDS_[bc546brl1]', 1)\n",
      "(u'TAB_e1_COL_WORDS_[bc546brl1g]', 1)\n",
      "(u'TAB_e1_COL_WORDS_[bc546bzl1g]', 1)\n",
      "(u'TAB_e1_COL_WORDS_[bc547arlg]', 1)\n",
      "(u'TAB_e1_COL_WORDS_[bc547azl1g]', 1)\n",
      "(u'TAB_e1_COL_WORDS_[bc547bg]', 1)\n",
      "(u'TAB_e1_COL_WORDS_[bc547brl1g]', 1)\n",
      "(u'TAB_e1_COL_WORDS_[bc547bzl1g]', 1)\n",
      "(u'TAB_e1_COL_WORDS_[bc547cg]', 1)\n",
      "(u'TAB_e1_COL_WORDS_[bc547czl1g]', 1)\n",
      "(u'TAB_e1_COL_WORDS_[bc548bg]', 1)\n",
      "(u'TAB_e1_COL_WORDS_[bc548brl1g]', 1)\n",
      "(u'TAB_e1_COL_WORDS_[bc548bzl1g]', 1)\n",
      "(u'TAB_e1_COL_WORDS_[bc548cg]', 1)\n",
      "(u'TAB_e1_COL_WORDS_[bc548czl1g]', 1)\n",
      "('VIZ_e1_PAGE_[5]', 1)\n",
      "(u'VIZ_e2_ALIGNED_npn', 1)\n",
      "(u'VIZ_e2_ALIGNED_LEFT_transistor', 1)\n",
      "(u'VIZ_e2_ALIGNED_transistor', 1)\n",
      "(u'VIZ_e2_ALIGNED_amplifier', 1)\n",
      "(u'VIZ_e2_ALIGNED_feature', 1)\n",
      "(u'VIZ_e2_ALIGNED_LEFT_amplifier', 1)\n",
      "(u'VIZ_e2_ALIGNED_LEFT_npn', 1)\n",
      "(u'VIZ_e2_ALIGNED_LEFT_transistors', 1)\n",
      "(u'VIZ_e2_ALIGNED_LEFT_feature', 1)\n",
      "(u'VIZ_e2_ALIGNED_transistors', 1)\n",
      "('VIZ_e2_PAGE_[1]', 1)\n"
     ]
    }
   ],
   "source": [
    "from fonduer.features.features import get_all_feats\n",
    "for f in get_all_feats(candidates[0]):\n",
    "    print f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Part_Attr\n"
     ]
    }
   ],
   "source": [
    "session.query(Part_Attr).filter(Part_Attr.split == 0).count()\n",
    "print isinstance(Part_Attr, type)\n",
    "print Part_Attr.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract with optimized postgres batch extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "Copying part_attr_feature to postgres\n",
      "COPY 4536\n",
      "\n",
      "CPU times: user 8.61 s, sys: 96 ms, total: 8.7 s\n",
      "Wall time: 15min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<4536x5681 sparse matrix of type '<type 'numpy.float32'>'\n",
       "\twith 804043 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fonduer.async_annotations import BatchFeatureAnnotator\n",
    "\n",
    "featurizer = BatchFeatureAnnotator(Part_Attr)\n",
    "%time F_train = featurizer.apply(split=0, parallelism=4)\n",
    "F_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract with default Snorkel extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    }
   ],
   "source": [
    "#from fonduer.async_annotations import \n",
    "from fonduer.annotations import FeatureAnnotator\n",
    "from fonduer.features.features import get_all_feats\n",
    "\n",
    "featurizer = FeatureAnnotator(f=get_all_feats)\n",
    "%time F_train = featurizer.apply(split=0, parallelism=4)\n",
    "F_train\n",
    "#print docs[0]\n",
    "#print session.query(Part_Attr).filter(Part_Attr.split == 0).filter(Part_Attr.part.sentence.document_id==531).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "from snorkel.utils import get_ORM_instance\n",
    "\n",
    "train = get_ORM_instance(CandidateSet, session, 'Hardware Train Candidates')\n",
    "dev   = get_ORM_instance(CandidateSet, session, 'Hardware Dev Candidates')\n",
    "test  = get_ORM_instance(CandidateSet, session, 'Hardware Test Candidates')\n",
    "\n",
    "if snorkel_postgres:\n",
    "    from fonduer.async_annotations import annotate\n",
    "    print \"Starting async featurization...\"\n",
    "    %time F_train = annotate(train, parallel=PARALLEL_F, dynamic_scheduling=False)\n",
    "    %time F_dev   = annotate(dev, parallel=PARALLEL_F, dynamic_scheduling=False,\\\n",
    "                             keyset = 'Hardware Train Candidates')\n",
    "    if TEST_SIZE:\n",
    "        %time F_test = annotate(test, parallel=PARALLEL_F, dynamic_scheduling=False,\\\n",
    "                                keyset = 'Hardware Train Candidates')\n",
    "    \n",
    "else:\n",
    "    from fonduer.models import CandidateSet\n",
    "    from fonduer.fast_annotations import FeatureManager\n",
    "    from snorkel.utils import get_ORM_instance\n",
    "\n",
    "    print \"Starting sync featurization...\"\n",
    "    feature_manager = FeatureManager()\n",
    "    %time F_train = feature_manager.create(session, train, 'Train Features')\n",
    "    %time F_dev = feature_manager.update(session, dev, 'Train Features', expand_key_set=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If necessary\n",
    "# import os\n",
    "# os.remove('snorkel.db');\n",
    "# os.system('cp snorkel.db\\ featurized snorkel.db');\n",
    "\n",
    "# from snorkel import SnorkelSession\n",
    "# session = SnorkelSession()\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(os.environ['SNORKELHOME'] + '/tutorials/tables/')\n",
    "\n",
    "# from snorkel.models import candidate_subclass\n",
    "# Part_Attr = candidate_subclass('Part_Attr', ['part','attr'])\n",
    "\n",
    "# from snorkel.models import CandidateSet\n",
    "# train = session.query(CandidateSet).filter(\n",
    "#     CandidateSet.name == 'Hardware Train Candidates').one()\n",
    "# dev = session.query(CandidateSet).filter(\n",
    "#     CandidateSet.name == 'Hardware Dev Candidates').one()\n",
    "\n",
    "# from snorkel.annotations import FeatureManager, LabelManager\n",
    "# feature_manager = FeatureManager()\n",
    "# %time F_train = feature_manager.load(session, train, 'Train Features')\n",
    "# %time F_dev = feature_manager.load(session, dev, 'Train Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hardware_lfs import get_lfs\n",
    "\n",
    "LFs = get_lfs(ATTRIBUTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for lf in LFs:\n",
    "    print lf(candidates[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fonduer.async_annotations import BatchLabelAnnotator\n",
    "labeler = BatchLabelAnnotator(Part_Attr, lfs = LFs)\n",
    "%time L_train=labeler.apply(split=0, parallelism=4)\n",
    "L_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess LF accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time L_train.lf_stats_legacy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If necessary:\n",
    "# import os\n",
    "# os.remove('snorkel.db');\n",
    "# os.system('cp snorkel.db\\ features snorkel.db');\n",
    "\n",
    "# from snorkel import SnorkelSession\n",
    "# session = SnorkelSession()\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(os.environ['SNORKELHOME'] + '/tutorials/tables/')\n",
    "\n",
    "# from snorkel.models import candidate_subclass\n",
    "# Part_Attr = candidate_subclass('Part_Attr', ['part','attr'])\n",
    "\n",
    "# from snorkel.models import CandidateSet\n",
    "# train = session.query(CandidateSet).filter(\n",
    "#     CandidateSet.name == 'Hardware Training Candidates').one()\n",
    "# dev = session.query(CandidateSet).filter(\n",
    "#     CandidateSet.name == 'Hardware Development Candidates').one()\n",
    "\n",
    "# from snorkel.annotations import FeatureManager, LabelManager\n",
    "# feature_manager = FeatureManager()\n",
    "# %time F_train = feature_manager.load(session, train, 'Train Features')\n",
    "# %time F_dev = feature_manager.load(session, dev, 'Train Features')\n",
    "\n",
    "# label_manager = LabelManager()\n",
    "# %time L_train = label_manager.load(session, train, 'LF Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "\n",
    "gen_model = GenerativeModel()\n",
    "%time gen_model.train(L_train, epochs=500, decay=0.95, step_size=0.1/L_train.shape[0], reg_param=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_model.weights.lf_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from snorkel.learning import NaiveBayes\n",
    "\n",
    "# gen_model = NaiveBayes()\n",
    "# %time gen_model.train(L_train, n_iter=2000, rate=1e-3, mu=1e-6)\n",
    "# train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(zip([lf.__name__ for lf in LFs], gen_model.weights.lf_accuracy_log_odds))\n",
    "print min(train_marginals)\n",
    "print max(train_marginals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import SparseLogisticRegression\n",
    "\n",
    "disc_model = SparseLogisticRegression()\n",
    "%time disc_model.train(F_train, train_marginals, n_epochs=200, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from snorkel.learning import LogReg\n",
    "\n",
    "# disc_model = LogReg()\n",
    "# %time disc_model.train(F_train, train_marginals, n_iter=10000, rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fonduer.annotations import LabelManager\n",
    "label_manager = LabelManager()\n",
    "L_dev = label_manager.load(session, dev, 'Hardware Dev Labels -- Gold')\n",
    "L_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = disc_model.score(session, F_dev, L_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fonduer.models import Corpus\n",
    "from hardware_utils import entity_level_f1\n",
    "import os\n",
    "\n",
    "gold_file = os.environ['SNORKELHOME'] + '/tutorials/fonduer/tables/data/hardware/dev/hardware_dev_gold.csv'\n",
    "corpus = session.query(Corpus).filter(Corpus.name == 'Hardware Dev').one()\n",
    "%time (TP, FP, FN) = entity_level_f1(tp.union(fp), gold_file, ATTRIBUTE, corpus, parts_by_doc=None)\n",
    "%time (TP, FP, FN) = entity_level_f1(tp.union(fp), gold_file, ATTRIBUTE, corpus, parts_by_doc=parts_by_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hardware_utils import get_gold_parts_by_doc, get_manual_parts_by_doc\n",
    "from snorkel.utils import get_ORM_instance\n",
    "from fonduer.models import Corpus\n",
    "\n",
    "parts_by_doc = get_gold_parts_by_doc()\n",
    "(TP, FP, FN) = entity_level_f1(tp.union(fp), gold_file, ATTRIBUTE, corpus, parts_by_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if TEST_SIZE:\n",
    "    from fonduer.annotations import LabelManager\n",
    "    label_manager = LabelManager()\n",
    "    L_test = label_manager.load(session, test, 'Hardware Test Labels -- Gold')\n",
    "    L_test.shape\n",
    "    \n",
    "    tp, fp, tn, fn = disc_model.score(session, F_test, L_test, b=0.91)\n",
    "    \n",
    "    from hardware_utils import get_gold_parts_by_doc, get_manual_parts_by_doc\n",
    "    from snorkel.utils import get_ORM_instance\n",
    "    from fonduer.models import Corpus\n",
    "\n",
    "    corpus = get_ORM_instance(Corpus, session, 'Hardware Test')\n",
    "\n",
    "    # parts_by_doc_test = get_manual_parts_by_doc(corpus.documents.all())\n",
    "    # parts_by_doc_test = None\n",
    "    import cPickle as pickle\n",
    "    pickle_file = os.environ['SNORKELHOME'] + '/tutorials/fonduer/tables/sandbox/parts_by_doc_test.pkl'\n",
    "    with open(pickle_file, 'r') as f:\n",
    "        parts_by_doc_test = pickle.load(f)\n",
    "\n",
    "    from hardware_utils import entity_level_f1\n",
    "\n",
    "    gold_file = os.environ['SNORKELHOME'] + '/tutorials/fonduer/tables/data/hardware/test/hardware_test_gold.csv'\n",
    "    (TP, FP, FN) = entity_level_f1(tp.union(fp), gold_file, ATTRIBUTE, corpus)\n",
    "    (TP, FP, FN) = entity_level_f1(tp.union(fp), gold_file, ATTRIBUTE, corpus, parts_by_doc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
