{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chemical-Disease Relation (CDR) Tutorial\n",
    "\n",
    "In this example, we'll be writing an application to extract *mentions of* **chemical-induced-disease relationships** from Pubmed abstracts, as per the [BioCreative CDR Challenge](http://www.biocreative.org/resources/corpora/biocreative-v-cdr-corpus/).  This tutorial will show off some of the more advanced features of Snorkel, so we'll assume you've followed the Intro tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reloading from the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "PARALLELISM = 6\n",
    "\n",
    "import os\n",
    "os.environ['SNORKELDB'] = 'postgres://localhost:5432/semparse_cdr'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\t8272 candidates\n",
      "Dev set:\t888 candidates\n",
      "Test set:\t4620 candidates\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "ChemicalDisease = candidate_subclass('ChemicalDisease', ['chemical', 'disease'])\n",
    "\n",
    "train = session.query(ChemicalDisease).filter(ChemicalDisease.split == 0).all()\n",
    "dev = session.query(ChemicalDisease).filter(ChemicalDisease.split == 1).all()\n",
    "test = session.query(ChemicalDisease).filter(ChemicalDisease.split == 2).all()\n",
    "\n",
    "print 'Training set:\\t{0} candidates'.format(len(train))\n",
    "print 'Dev set:\\t{0} candidates'.format(len(dev))\n",
    "print 'Test set:\\t{0} candidates'.format(len(test))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_marginals\n",
    "train_marginals = load_marginals(session, split=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (NEW) Featurize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureAnnotator\n",
    "featurizer = FeatureAnnotator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %time F_train = featurizer.apply(split=0, parallelism=PARALLELISM)\n",
    "# F_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %time F_dev  = featurizer.apply_existing(split=1, parallelism=PARALLELISM)\n",
    "# F_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %time F_test = featurizer.apply_existing(split=2, parallelism=PARALLELISM)\n",
    "# F_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8272x122840 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 448906 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_train = featurizer.load_matrix(session, split=0)\n",
    "F_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<888x122840 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 27734 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_dev = featurizer.load_matrix(session, split=1)\n",
    "F_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4620x122840 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 138487 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_test = featurizer.load_matrix(session, split=2)\n",
    "F_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_train = load_gold_labels(session, annotator_name='gold', split=0)\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import SparseLogisticRegression\n",
    "disc_model = SparseLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RandomSearch search of size 20. Search space size = 125.\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning.utils import MentionScorer\n",
    "from snorkel.learning import RandomSearch, ListParameter, RangeParameter\n",
    "\n",
    "# Searching over learning rate\n",
    "rate_param = RangeParameter('lr', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l1_param  = RangeParameter('l1_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l2_param  = RangeParameter('l2_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "\n",
    "searcher = RandomSearch(session, disc_model, F_train, train_marginals, [rate_param, l1_param, l2_param], n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "[1] Testing lr = 1.00e-04, l1_penalty = 1.00e-03, l2_penalty = 1.00e-04\n",
      "============================================================\n",
      "[SparseLR] lr=0.0001 l1=0.001 l2=0.0001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=3686  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (1.01s)\tAvg. loss=0.690457\tNNZ=122840\n",
      "[SparseLR] Epoch 25 (23.38s)\tAvg. loss=-0.241990\tNNZ=122840\n",
      "[SparseLR] Epoch 49 (46.38s)\tAvg. loss=-1.055238\tNNZ=122840\n",
      "[SparseLR] Training done (46.38s)\n",
      "[SparseLR] Model saved. To load, use name\n",
      "\t\tSparseLR_0\n",
      "============================================================\n",
      "[2] Testing lr = 1.00e-05, l1_penalty = 1.00e-06, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=1e-05 l1=1e-06 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=3686  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (1.03s)\tAvg. loss=0.691683\tNNZ=122840\n",
      "[SparseLR] Epoch 25 (23.28s)\tAvg. loss=0.595268\tNNZ=122840\n",
      "[SparseLR] Epoch 49 (44.58s)\tAvg. loss=0.503365\tNNZ=122840\n",
      "[SparseLR] Training done (44.58s)\n",
      "============================================================\n",
      "[3] Testing lr = 1.00e-06, l1_penalty = 1.00e-03, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=1e-06 l1=0.001 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=3686  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (1.02s)\tAvg. loss=0.713495\tNNZ=122840\n",
      "[SparseLR] Epoch 25 (23.26s)\tAvg. loss=0.703580\tNNZ=122840\n",
      "[SparseLR] Epoch 49 (45.27s)\tAvg. loss=0.694024\tNNZ=122840\n",
      "[SparseLR] Training done (45.27s)\n",
      "============================================================\n",
      "[4] Testing lr = 1.00e-02, l1_penalty = 1.00e-04, l2_penalty = 1.00e-06\n",
      "============================================================\n",
      "[SparseLR] lr=0.01 l1=0.0001 l2=1e-06\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=3686  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (1.06s)\tAvg. loss=0.770879\tNNZ=122840\n",
      "[SparseLR] Epoch 25 (23.45s)\tAvg. loss=-70.281494\tNNZ=122840\n",
      "[SparseLR] Epoch 49 (44.93s)\tAvg. loss=-139.620482\tNNZ=122840\n",
      "[SparseLR] Training done (44.93s)\n",
      "[SparseLR] Model saved. To load, use name\n",
      "\t\tSparseLR_3\n",
      "============================================================\n",
      "[5] Testing lr = 1.00e-06, l1_penalty = 1.00e-06, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=1e-06 l1=1e-06 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=3686  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (1.03s)\tAvg. loss=0.688437\tNNZ=122840\n",
      "[SparseLR] Epoch 25 (22.78s)\tAvg. loss=0.678480\tNNZ=122840\n",
      "[SparseLR] Epoch 49 (44.43s)\tAvg. loss=0.668850\tNNZ=122840\n",
      "[SparseLR] Training done (44.43s)\n",
      "============================================================\n",
      "[6] Testing lr = 1.00e-05, l1_penalty = 1.00e-02, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=1e-05 l1=0.01 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=3686  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (1.03s)\tAvg. loss=0.781688\tNNZ=122840\n",
      "[SparseLR] Epoch 25 (23.40s)\tAvg. loss=0.655172\tNNZ=122840\n",
      "[SparseLR] Epoch 49 (45.61s)\tAvg. loss=0.556746\tNNZ=122840\n",
      "[SparseLR] Training done (45.61s)\n",
      "[SparseLR] Model saved. To load, use name\n",
      "\t\tSparseLR_5\n",
      "============================================================\n",
      "[7] Testing lr = 1.00e-06, l1_penalty = 1.00e-04, l2_penalty = 1.00e-05\n",
      "============================================================\n",
      "[SparseLR] lr=1e-06 l1=0.0001 l2=1e-05\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=3686  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (1.10s)\tAvg. loss=0.695324\tNNZ=122840\n",
      "[SparseLR] Epoch 25 (24.53s)\tAvg. loss=0.685669\tNNZ=122840\n",
      "[SparseLR] Epoch 49 (47.08s)\tAvg. loss=0.676346\tNNZ=122840\n",
      "[SparseLR] Training done (47.08s)\n",
      "============================================================\n",
      "[8] Testing lr = 1.00e-02, l1_penalty = 1.00e-06, l2_penalty = 1.00e-06\n",
      "============================================================\n",
      "[SparseLR] lr=0.01 l1=1e-06 l2=1e-06\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=3686  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (1.06s)\tAvg. loss=0.784276\tNNZ=122840\n",
      "[SparseLR] Epoch 25 (24.05s)\tAvg. loss=-70.641915\tNNZ=122840\n",
      "[SparseLR] Epoch 49 (46.41s)\tAvg. loss=-140.405743\tNNZ=122840\n",
      "[SparseLR] Training done (46.41s)\n",
      "============================================================\n",
      "[9] Testing lr = 1.00e-03, l1_penalty = 1.00e-06, l2_penalty = 1.00e-02\n",
      "============================================================\n",
      "[SparseLR] lr=0.001 l1=1e-06 l2=0.01\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=3686  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (1.07s)\tAvg. loss=0.700721\tNNZ=122840\n",
      "[SparseLR] Epoch 25 (23.77s)\tAvg. loss=-6.854826\tNNZ=122840\n",
      "[SparseLR] Epoch 49 (46.37s)\tAvg. loss=-13.580398\tNNZ=122840\n",
      "[SparseLR] Training done (46.37s)\n",
      "[SparseLR] Model saved. To load, use name\n",
      "\t\tSparseLR_8\n",
      "============================================================\n",
      "[10] Testing lr = 1.00e-03, l1_penalty = 1.00e-05, l2_penalty = 1.00e-04\n",
      "============================================================\n",
      "[SparseLR] lr=0.001 l1=1e-05 l2=0.0001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=3686  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (1.11s)\tAvg. loss=0.701480\tNNZ=122840\n",
      "[SparseLR] Epoch 25 (24.18s)\tAvg. loss=-6.720786\tNNZ=122840\n",
      "[SparseLR] Epoch 49 (45.99s)\tAvg. loss=-13.533750\tNNZ=122840\n",
      "[SparseLR] Training done (45.99s)\n",
      "============================================================\n",
      "[11] Testing lr = 1.00e-06, l1_penalty = 1.00e-02, l2_penalty = 1.00e-05\n",
      "============================================================\n",
      "[SparseLR] lr=1e-06 l1=0.01 l2=1e-05\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=3686  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (1.07s)\tAvg. loss=0.793009\tNNZ=122840\n",
      "[SparseLR] Epoch 25 (23.98s)\tAvg. loss=0.778688\tNNZ=122840\n",
      "[SparseLR] Epoch 49 (46.66s)\tAvg. loss=0.765231\tNNZ=122840\n",
      "[SparseLR] Training done (46.66s)\n",
      "============================================================\n",
      "[12] Testing lr = 1.00e-05, l1_penalty = 1.00e-06, l2_penalty = 1.00e-02\n",
      "============================================================\n",
      "[SparseLR] lr=1e-05 l1=1e-06 l2=0.01\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=3686  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (1.16s)\tAvg. loss=0.692637\tNNZ=122840\n",
      "[SparseLR] Epoch 25 (23.13s)\tAvg. loss=0.597492\tNNZ=122840\n",
      "[SparseLR] Epoch 49 (44.09s)\tAvg. loss=0.506696\tNNZ=122840\n",
      "[SparseLR] Training done (44.09s)\n",
      "============================================================\n",
      "[13] Testing lr = 1.00e-02, l1_penalty = 1.00e-05, l2_penalty = 1.00e-06\n",
      "============================================================\n",
      "[SparseLR] lr=0.01 l1=1e-05 l2=1e-06\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=3686  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (1.09s)\tAvg. loss=0.780272\tNNZ=122840\n",
      "[SparseLR] Epoch 25 (23.78s)\tAvg. loss=-72.467044\tNNZ=122840\n",
      "[SparseLR] Epoch 49 (45.46s)\tAvg. loss=-143.919809\tNNZ=122840\n",
      "[SparseLR] Training done (45.46s)\n",
      "============================================================\n",
      "[14] Testing lr = 1.00e-02, l1_penalty = 1.00e-03, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=0.01 l1=0.001 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=3686  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (1.10s)\tAvg. loss=0.790566\tNNZ=122840\n",
      "[SparseLR] Epoch 25 (23.22s)\tAvg. loss=-70.618199\tNNZ=122840\n",
      "[SparseLR] Epoch 49 (44.68s)\tAvg. loss=-138.124633\tNNZ=122840\n",
      "[SparseLR] Training done (44.68s)\n",
      "============================================================\n",
      "[15] Testing lr = 1.00e-04, l1_penalty = 1.00e-05, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=0.0001 l1=1e-05 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=3686  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (1.12s)\tAvg. loss=0.691095\tNNZ=122840\n",
      "[SparseLR] Epoch 25 (23.99s)\tAvg. loss=-0.222572\tNNZ=122840\n",
      "[SparseLR] Epoch 49 (46.38s)\tAvg. loss=-1.025329\tNNZ=122840\n",
      "[SparseLR] Training done (46.38s)\n",
      "============================================================\n",
      "[16] Testing lr = 1.00e-03, l1_penalty = 1.00e-05, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=0.001 l1=1e-05 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=3686  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (1.15s)\tAvg. loss=0.694568\tNNZ=122840\n",
      "[SparseLR] Epoch 25 (24.00s)\tAvg. loss=-6.896352\tNNZ=122840\n",
      "[SparseLR] Epoch 49 (46.30s)\tAvg. loss=-13.850526\tNNZ=122840\n",
      "[SparseLR] Training done (46.30s)\n",
      "============================================================\n",
      "[17] Testing lr = 1.00e-03, l1_penalty = 1.00e-02, l2_penalty = 1.00e-02\n",
      "============================================================\n",
      "[SparseLR] lr=0.001 l1=0.01 l2=0.01\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=3686  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (1.20s)\tAvg. loss=0.741032\tNNZ=122840\n",
      "[SparseLR] Epoch 25 (24.52s)\tAvg. loss=-5.229540\tNNZ=122840\n",
      "[SparseLR] Epoch 49 (46.66s)\tAvg. loss=-10.467485\tNNZ=122840\n",
      "[SparseLR] Training done (46.66s)\n",
      "============================================================\n",
      "[18] Testing lr = 1.00e-02, l1_penalty = 1.00e-03, l2_penalty = 1.00e-06\n",
      "============================================================\n",
      "[SparseLR] lr=0.01 l1=0.001 l2=1e-06\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=3686  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (1.18s)\tAvg. loss=0.775994\tNNZ=122840\n",
      "[SparseLR] Epoch 25 (24.52s)\tAvg. loss=-68.162001\tNNZ=122840\n",
      "[SparseLR] Epoch 49 (47.03s)\tAvg. loss=-135.454141\tNNZ=122840\n",
      "[SparseLR] Training done (47.03s)\n",
      "============================================================\n",
      "[19] Testing lr = 1.00e-06, l1_penalty = 1.00e-06, l2_penalty = 1.00e-04\n",
      "============================================================\n",
      "[SparseLR] lr=1e-06 l1=1e-06 l2=0.0001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=3686  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (1.25s)\tAvg. loss=0.701912\tNNZ=122840\n",
      "[SparseLR] Epoch 25 (24.18s)\tAvg. loss=0.692096\tNNZ=122840\n",
      "[SparseLR] Epoch 49 (46.85s)\tAvg. loss=0.682603\tNNZ=122840\n",
      "[SparseLR] Training done (46.85s)\n",
      "============================================================\n",
      "[20] Testing lr = 1.00e-03, l1_penalty = 1.00e-02, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=0.001 l1=0.01 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model\n",
      "[SparseLR] #examples=3686  #epochs=50  batch size=100\n",
      "[SparseLR] Epoch 0 (1.19s)\tAvg. loss=0.739876\tNNZ=122840\n",
      "[SparseLR] Epoch 25 (24.31s)\tAvg. loss=-5.301605\tNNZ=122840\n",
      "[SparseLR] Epoch 49 (46.53s)\tAvg. loss=-10.735886\tNNZ=122840\n",
      "[SparseLR] Training done (46.53s)\n",
      "[SparseLR] Loaded model <SparseLR_8>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>l1_penalty</th>\n",
       "      <th>l2_penalty</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>Rec.</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.372240</td>\n",
       "      <td>0.797297</td>\n",
       "      <td>0.507527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.379195</td>\n",
       "      <td>0.763514</td>\n",
       "      <td>0.506726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.354430</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.500497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.359584</td>\n",
       "      <td>0.817568</td>\n",
       "      <td>0.499484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.343381</td>\n",
       "      <td>0.885135</td>\n",
       "      <td>0.494806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.343666</td>\n",
       "      <td>0.861486</td>\n",
       "      <td>0.491329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.342896</td>\n",
       "      <td>0.847973</td>\n",
       "      <td>0.488327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.341001</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.486957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.349702</td>\n",
       "      <td>0.793919</td>\n",
       "      <td>0.485537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.344239</td>\n",
       "      <td>0.817568</td>\n",
       "      <td>0.484484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.337317</td>\n",
       "      <td>0.858108</td>\n",
       "      <td>0.484271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.339650</td>\n",
       "      <td>0.787162</td>\n",
       "      <td>0.474542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.338658</td>\n",
       "      <td>0.716216</td>\n",
       "      <td>0.459870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.330289</td>\n",
       "      <td>0.733108</td>\n",
       "      <td>0.455404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.327217</td>\n",
       "      <td>0.722973</td>\n",
       "      <td>0.450526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.352250</td>\n",
       "      <td>0.608108</td>\n",
       "      <td>0.446097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.331349</td>\n",
       "      <td>0.564189</td>\n",
       "      <td>0.417500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.339827</td>\n",
       "      <td>0.530405</td>\n",
       "      <td>0.414248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.376582</td>\n",
       "      <td>0.402027</td>\n",
       "      <td>0.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.305263</td>\n",
       "      <td>0.391892</td>\n",
       "      <td>0.343195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lr  l1_penalty  l2_penalty     Prec.      Rec.        F1\n",
       "8   0.001000    0.000001    0.010000  0.372240  0.797297  0.507527\n",
       "5   0.000010    0.010000    0.001000  0.379195  0.763514  0.506726\n",
       "15  0.001000    0.000010    0.001000  0.354430  0.851351  0.500497\n",
       "3   0.010000    0.000100    0.000001  0.359584  0.817568  0.499484\n",
       "17  0.010000    0.001000    0.000001  0.343381  0.885135  0.494806\n",
       "14  0.000100    0.000010    0.001000  0.343666  0.861486  0.491329\n",
       "13  0.010000    0.001000    0.001000  0.342896  0.847973  0.488327\n",
       "16  0.001000    0.010000    0.010000  0.341001  0.851351  0.486957\n",
       "12  0.010000    0.000010    0.000001  0.349702  0.793919  0.485537\n",
       "19  0.001000    0.010000    0.001000  0.344239  0.817568  0.484484\n",
       "7   0.010000    0.000001    0.000001  0.337317  0.858108  0.484271\n",
       "0   0.000100    0.001000    0.000100  0.339650  0.787162  0.474542\n",
       "1   0.000010    0.000001    0.001000  0.338658  0.716216  0.459870\n",
       "9   0.001000    0.000010    0.000100  0.330289  0.733108  0.455404\n",
       "10  0.000001    0.010000    0.000010  0.327217  0.722973  0.450526\n",
       "6   0.000001    0.000100    0.000010  0.352250  0.608108  0.446097\n",
       "11  0.000010    0.000001    0.010000  0.331349  0.564189  0.417500\n",
       "4   0.000001    0.000001    0.001000  0.339827  0.530405  0.414248\n",
       "18  0.000001    0.000001    0.000100  0.376582  0.402027  0.388889\n",
       "2   0.000001    0.001000    0.001000  0.305263  0.391892  0.343195"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time searcher.fit(F_dev, L_gold_dev, n_epochs=50, rebalance=True, print_freq=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.797\n",
      "Neg. class accuracy: 0.328\n",
      "Precision            0.372\n",
      "Recall               0.797\n",
      "F1                   0.508\n",
      "----------------------------------------\n",
      "TP: 236 | FP: 398 | TN: 194 | FN: 60\n",
      "========================================\n",
      "\n",
      "CPU times: user 555 ms, sys: 13.5 ms, total: 569 ms\n",
      "Wall time: 646 ms\n"
     ]
    }
   ],
   "source": [
    "%time TP, FP, TN, FN = disc_model.score(session, F_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnotatorLabels created: 0\n"
     ]
    }
   ],
   "source": [
    "from load_external_annotations import load_external_labels\n",
    "load_external_labels(session, ChemicalDisease, split=2, annotator='gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4620x1 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 4620 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)\n",
    "L_gold_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.789\n",
      "Neg. class accuracy: 0.253\n",
      "Precision            0.338\n",
      "Recall               0.789\n",
      "F1                   0.473\n",
      "----------------------------------------\n",
      "TP: 1188 | FP: 2328 | TN: 787 | FN: 317\n",
      "========================================\n",
      "\n",
      "CPU times: user 8.76 s, sys: 229 ms, total: 8.99 s\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%time _, _, _, _ = disc_model.score(session, F_test, L_gold_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Test results a/f pull of new master:\n",
    "========================================\n",
    "Scores (Un-adjusted)\n",
    "========================================\n",
    "Pos. class accuracy: 0.789\n",
    "Neg. class accuracy: 0.253\n",
    "Precision            0.338\n",
    "Recall               0.789\n",
    "F1                   0.473\n",
    "----------------------------------------\n",
    "TP: 1188 | FP: 2328 | TN: 787 | FN: 317\n",
    "========================================"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Test results b/f pull of new master:\n",
    "========================================\n",
    "Scores (Un-adjusted)\n",
    "========================================\n",
    "Pos. class accuracy: 0.969\n",
    "Neg. class accuracy: 0.0347\n",
    "Precision            0.327\n",
    "Recall               0.969\n",
    "F1                   0.488\n",
    "----------------------------------------\n",
    "TP: 1458 | FP: 3007 | TN: 108 | FN: 47\n",
    "========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V: Training an LSTM\n",
    "In the intro tutorial, we automatically featurized the candidates and trained a linear model over these features. Here, we'll train a more complicated model for relation extraction: an LSTM network. You can read more about LSTMs [here](https://en.wikipedia.org/wiki/Long_short-term_memory) or [here](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). An LSTM is a type of recurrent neural network and automatically generates a numerical representation for the candidate based on the sentence text, so no need for featurizing explicitly as in the intro tutorial. LSTMs take longer to train, and Snorkel doesn't currently support hyperparameter searches for them. We'll train a single model here, but feel free to try out other parameter sets. Just make sure to use the development set - and not the test set - for model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from snorkel.contrib.learning import reLSTM\n",
    "\n",
    "# lstm = reLSTM()\n",
    "# %time lstm.train(\n",
    "#     train, train_marginals, lr=0.005, dim=200, n_epochs=30,\n",
    "#     dropout_rate=0.5, rebalance=0.25, print_freq=5\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring on the test set\n",
    "\n",
    "Finally, we'll evaluate our performance on the blind test set of 500 documents. We'll load labels similar to how we did for the development set, and use the `score` function of our extraction model to see how we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from load_external_annotations import load_external_labels\n",
    "# load_external_labels(session, ChemicalDisease, split=2, annotator='gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from snorkel.annotations import load_gold_labels\n",
    "# L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)\n",
    "# L_gold_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %time _, _, _, _ = lstm.score(session, test, L_gold_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
