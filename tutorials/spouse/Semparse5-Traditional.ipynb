{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V: Traditional Supervision\n",
    "\n",
    "We will compare the performance of:\n",
    "\n",
    "(a) traditional supervision - positive and negative labels on examples\n",
    "\n",
    "(b) natural language supervision - explanations converted into LFs, which are then denoised and applied to unlabeled data to create a much larger but noisy training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "os.environ['SNORKELDB'] = 'postgres:///semparse'\n",
    "\n",
    "import numpy as np\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat our definition of the `Spouse` `Candidate` subclass from Parts II and III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "Spouse = candidate_subclass('Spouse', ['person1', 'person2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureAnnotator\n",
    "featurizer = FeatureAnnotator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F_train = featurizer.load_matrix(session, split=0)\n",
    "F_dev   = featurizer.load_matrix(session, split=1)\n",
    "F_unlabled   = featurizer.load_matrix(session, split=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_train = load_gold_labels(session, annotator_name='gold', split=0)\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Traditional Supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import SparseLogisticRegression\n",
    "disc_model = SparseLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(129,)\n"
     ]
    }
   ],
   "source": [
    "train_marginals = np.array(L_gold_train.todense()).reshape((L_gold_train.shape[0],))\n",
    "# convert -1s to 0s; NOTE: there were already some 0's; this could cause weirdness\n",
    "train_marginals[train_marginals==-1] = 0\n",
    "print train_marginals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RandomSearch search of size 20. Search space size = 125.\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning.utils import MentionScorer\n",
    "from snorkel.learning import RandomSearch, ListParameter, RangeParameter\n",
    "\n",
    "# Searching over learning rate\n",
    "rate_param = RangeParameter('lr', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l1_param  = RangeParameter('l1_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l2_param  = RangeParameter('l2_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "\n",
    "searcher = RandomSearch(session, disc_model, F_train, train_marginals, [rate_param, l1_param, l2_param], n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "[1] Testing lr = 1.00e-02, l1_penalty = 1.00e-03, l2_penalty = 1.00e-04\n",
      "============================================================\n",
      "[SparseLR] lr=0.01 l1=0.001 l2=0.0001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=4\n",
      "[SparseLR] Epoch 0 (0.09s)\tAvg. loss=0.517689\tNNZ=4044\n",
      "[SparseLR] Epoch 25 (0.14s)\tAvg. loss=0.153034\tNNZ=4044\n",
      "[SparseLR] Epoch 49 (0.18s)\tAvg. loss=0.085940\tNNZ=4044\n",
      "[SparseLR] Training done (0.18s)\n",
      "[SparseLR] Model saved. To load, use name\n",
      "\t\tSparseLR_0\n",
      "============================================================\n",
      "[2] Testing lr = 1.00e-04, l1_penalty = 1.00e-06, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=0.0001 l1=1e-06 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=4\n",
      "[SparseLR] Epoch 0 (0.07s)\tAvg. loss=0.520776\tNNZ=4044\n",
      "[SparseLR] Epoch 25 (0.12s)\tAvg. loss=0.503827\tNNZ=4044\n",
      "[SparseLR] Epoch 49 (0.16s)\tAvg. loss=0.488437\tNNZ=4044\n",
      "[SparseLR] Training done (0.16s)\n",
      "[SparseLR] Model saved. To load, use name\n",
      "\t\tSparseLR_1\n",
      "============================================================\n",
      "[3] Testing lr = 1.00e-03, l1_penalty = 1.00e-05, l2_penalty = 1.00e-05\n",
      "============================================================\n",
      "[SparseLR] lr=0.001 l1=1e-05 l2=1e-05\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=4\n",
      "[SparseLR] Epoch 0 (0.14s)\tAvg. loss=0.518592\tNNZ=4044\n",
      "[SparseLR] Epoch 25 (0.19s)\tAvg. loss=0.342856\tNNZ=4044\n",
      "[SparseLR] Epoch 49 (0.25s)\tAvg. loss=0.251590\tNNZ=4044\n",
      "[SparseLR] Training done (0.25s)\n",
      "============================================================\n",
      "[4] Testing lr = 1.00e-03, l1_penalty = 1.00e-06, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=0.001 l1=1e-06 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=4\n",
      "[SparseLR] Epoch 0 (0.12s)\tAvg. loss=0.508537\tNNZ=4044\n",
      "[SparseLR] Epoch 25 (0.16s)\tAvg. loss=0.351684\tNNZ=4044\n",
      "[SparseLR] Epoch 49 (0.20s)\tAvg. loss=0.269388\tNNZ=4044\n",
      "[SparseLR] Training done (0.20s)\n",
      "============================================================\n",
      "[5] Testing lr = 1.00e-02, l1_penalty = 1.00e-04, l2_penalty = 1.00e-05\n",
      "============================================================\n",
      "[SparseLR] lr=0.01 l1=0.0001 l2=1e-05\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=4\n",
      "[SparseLR] Epoch 0 (0.09s)\tAvg. loss=0.499596\tNNZ=4044\n",
      "[SparseLR] Epoch 25 (0.13s)\tAvg. loss=0.096144\tNNZ=4044\n",
      "[SparseLR] Epoch 49 (0.18s)\tAvg. loss=0.052374\tNNZ=4044\n",
      "[SparseLR] Training done (0.18s)\n",
      "============================================================\n",
      "[6] Testing lr = 1.00e-06, l1_penalty = 1.00e-03, l2_penalty = 1.00e-05\n",
      "============================================================\n",
      "[SparseLR] lr=1e-06 l1=0.001 l2=1e-05\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=4\n",
      "[SparseLR] Epoch 0 (0.09s)\tAvg. loss=0.529384\tNNZ=4044\n",
      "[SparseLR] Epoch 25 (0.12s)\tAvg. loss=0.529728\tNNZ=4044\n",
      "[SparseLR] Epoch 49 (0.16s)\tAvg. loss=0.529532\tNNZ=4044\n",
      "[SparseLR] Training done (0.16s)\n",
      "============================================================\n",
      "[7] Testing lr = 1.00e-06, l1_penalty = 1.00e-03, l2_penalty = 1.00e-02\n",
      "============================================================\n",
      "[SparseLR] lr=1e-06 l1=0.001 l2=0.01\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=4\n",
      "[SparseLR] Epoch 0 (0.11s)\tAvg. loss=0.510676\tNNZ=4044\n",
      "[SparseLR] Epoch 25 (0.16s)\tAvg. loss=0.513754\tNNZ=4044\n",
      "[SparseLR] Epoch 49 (0.20s)\tAvg. loss=0.513565\tNNZ=4044\n",
      "[SparseLR] Training done (0.20s)\n",
      "============================================================\n",
      "[8] Testing lr = 1.00e-02, l1_penalty = 1.00e-05, l2_penalty = 1.00e-02\n",
      "============================================================\n",
      "[SparseLR] lr=0.01 l1=1e-05 l2=0.01\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=4\n",
      "[SparseLR] Epoch 0 (0.12s)\tAvg. loss=0.533016\tNNZ=4044\n",
      "[SparseLR] Epoch 25 (0.17s)\tAvg. loss=0.092552\tNNZ=4044\n",
      "[SparseLR] Epoch 49 (0.22s)\tAvg. loss=0.048145\tNNZ=4044\n",
      "[SparseLR] Training done (0.22s)\n",
      "============================================================\n",
      "[9] Testing lr = 1.00e-04, l1_penalty = 1.00e-02, l2_penalty = 1.00e-06\n",
      "============================================================\n",
      "[SparseLR] lr=0.0001 l1=0.01 l2=1e-06\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=4\n",
      "[SparseLR] Epoch 0 (0.12s)\tAvg. loss=0.535498\tNNZ=4044\n",
      "[SparseLR] Epoch 25 (0.16s)\tAvg. loss=0.509275\tNNZ=4044\n",
      "[SparseLR] Epoch 49 (0.19s)\tAvg. loss=0.485851\tNNZ=4044\n",
      "[SparseLR] Training done (0.19s)\n",
      "[SparseLR] Model saved. To load, use name\n",
      "\t\tSparseLR_8\n",
      "============================================================\n",
      "[10] Testing lr = 1.00e-06, l1_penalty = 1.00e-05, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=1e-06 l1=1e-05 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=4\n",
      "[SparseLR] Epoch 0 (0.12s)\tAvg. loss=0.501079\tNNZ=4044\n",
      "[SparseLR] Epoch 25 (0.16s)\tAvg. loss=0.500860\tNNZ=4044\n",
      "[SparseLR] Epoch 49 (0.20s)\tAvg. loss=0.500651\tNNZ=4044\n",
      "[SparseLR] Training done (0.20s)\n",
      "============================================================\n",
      "[11] Testing lr = 1.00e-04, l1_penalty = 1.00e-02, l2_penalty = 1.00e-04\n",
      "============================================================\n",
      "[SparseLR] lr=0.0001 l1=0.01 l2=0.0001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=4\n",
      "[SparseLR] Epoch 0 (0.12s)\tAvg. loss=0.512407\tNNZ=4044\n",
      "[SparseLR] Epoch 25 (0.16s)\tAvg. loss=0.483195\tNNZ=4044\n",
      "[SparseLR] Epoch 49 (0.21s)\tAvg. loss=0.460736\tNNZ=4044\n",
      "[SparseLR] Training done (0.21s)\n",
      "============================================================\n",
      "[12] Testing lr = 1.00e-03, l1_penalty = 1.00e-06, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=0.001 l1=1e-06 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=4\n",
      "[SparseLR] Epoch 0 (0.12s)\tAvg. loss=0.529608\tNNZ=4044\n",
      "[SparseLR] Epoch 25 (0.16s)\tAvg. loss=0.359598\tNNZ=4044\n",
      "[SparseLR] Epoch 49 (0.20s)\tAvg. loss=0.266338\tNNZ=4044\n",
      "[SparseLR] Training done (0.20s)\n",
      "============================================================\n",
      "[13] Testing lr = 1.00e-04, l1_penalty = 1.00e-04, l2_penalty = 1.00e-05\n",
      "============================================================\n",
      "[SparseLR] lr=0.0001 l1=0.0001 l2=1e-05\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=4\n",
      "[SparseLR] Epoch 0 (0.14s)\tAvg. loss=0.549308\tNNZ=4044\n",
      "[SparseLR] Epoch 25 (0.19s)\tAvg. loss=0.496962\tNNZ=4044\n",
      "[SparseLR] Epoch 49 (0.23s)\tAvg. loss=0.456156\tNNZ=4044\n",
      "[SparseLR] Training done (0.23s)\n",
      "============================================================\n",
      "[14] Testing lr = 1.00e-03, l1_penalty = 1.00e-05, l2_penalty = 1.00e-04\n",
      "============================================================\n",
      "[SparseLR] lr=0.001 l1=1e-05 l2=0.0001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=4\n",
      "[SparseLR] Epoch 0 (0.13s)\tAvg. loss=0.520331\tNNZ=4044\n",
      "[SparseLR] Epoch 25 (0.16s)\tAvg. loss=0.397003\tNNZ=4044\n",
      "[SparseLR] Epoch 49 (0.20s)\tAvg. loss=0.319407\tNNZ=4044\n",
      "[SparseLR] Training done (0.20s)\n",
      "[SparseLR] Model saved. To load, use name\n",
      "\t\tSparseLR_13\n",
      "============================================================\n",
      "[15] Testing lr = 1.00e-04, l1_penalty = 1.00e-02, l2_penalty = 1.00e-05\n",
      "============================================================\n",
      "[SparseLR] lr=0.0001 l1=0.01 l2=1e-05\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=4\n",
      "[SparseLR] Epoch 0 (0.44s)\tAvg. loss=0.514323\tNNZ=4044\n",
      "[SparseLR] Epoch 25 (0.49s)\tAvg. loss=0.497345\tNNZ=4044\n",
      "[SparseLR] Epoch 49 (0.53s)\tAvg. loss=0.481922\tNNZ=4044\n",
      "[SparseLR] Training done (0.53s)\n",
      "============================================================\n",
      "[16] Testing lr = 1.00e-05, l1_penalty = 1.00e-06, l2_penalty = 1.00e-04\n",
      "============================================================\n",
      "[SparseLR] lr=1e-05 l1=1e-06 l2=0.0001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=4\n",
      "[SparseLR] Epoch 0 (0.14s)\tAvg. loss=0.516159\tNNZ=4044\n",
      "[SparseLR] Epoch 25 (0.18s)\tAvg. loss=0.512726\tNNZ=4044\n",
      "[SparseLR] Epoch 49 (0.23s)\tAvg. loss=0.509464\tNNZ=4044\n",
      "[SparseLR] Training done (0.23s)\n",
      "============================================================\n",
      "[17] Testing lr = 1.00e-06, l1_penalty = 1.00e-02, l2_penalty = 1.00e-03\n",
      "============================================================\n",
      "[SparseLR] lr=1e-06 l1=0.01 l2=0.001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=4\n",
      "[SparseLR] Epoch 0 (0.16s)\tAvg. loss=0.513022\tNNZ=4044\n",
      "[SparseLR] Epoch 25 (0.22s)\tAvg. loss=0.517910\tNNZ=4044\n",
      "[SparseLR] Epoch 49 (0.27s)\tAvg. loss=0.517537\tNNZ=4044\n",
      "[SparseLR] Training done (0.27s)\n",
      "============================================================\n",
      "[18] Testing lr = 1.00e-02, l1_penalty = 1.00e-05, l2_penalty = 1.00e-06\n",
      "============================================================\n",
      "[SparseLR] lr=0.01 l1=1e-05 l2=1e-06\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=4\n",
      "[SparseLR] Epoch 0 (0.17s)\tAvg. loss=0.508095\tNNZ=4044\n",
      "[SparseLR] Epoch 25 (0.21s)\tAvg. loss=0.095775\tNNZ=4044\n",
      "[SparseLR] Epoch 49 (0.26s)\tAvg. loss=0.051513\tNNZ=4044\n",
      "[SparseLR] Training done (0.26s)\n",
      "============================================================\n",
      "[19] Testing lr = 1.00e-05, l1_penalty = 1.00e-02, l2_penalty = 1.00e-06\n",
      "============================================================\n",
      "[SparseLR] lr=1e-05 l1=0.01 l2=1e-06\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=4\n",
      "[SparseLR] Epoch 0 (0.16s)\tAvg. loss=0.544477\tNNZ=4044\n",
      "[SparseLR] Epoch 25 (0.21s)\tAvg. loss=0.542247\tNNZ=4044\n",
      "[SparseLR] Epoch 49 (0.26s)\tAvg. loss=0.540118\tNNZ=4044\n",
      "[SparseLR] Training done (0.26s)\n",
      "============================================================\n",
      "[20] Testing lr = 1.00e-02, l1_penalty = 1.00e-05, l2_penalty = 1.00e-04\n",
      "============================================================\n",
      "[SparseLR] lr=0.01 l1=1e-05 l2=0.0001\n",
      "[SparseLR] Building model\n",
      "[SparseLR] Training model  #epochs=50  batch=4\n",
      "[SparseLR] Epoch 0 (0.18s)\tAvg. loss=0.518043\tNNZ=4044\n",
      "[SparseLR] Epoch 25 (0.22s)\tAvg. loss=0.132325\tNNZ=4044\n",
      "[SparseLR] Epoch 49 (0.27s)\tAvg. loss=0.073257\tNNZ=4044\n",
      "[SparseLR] Training done (0.27s)\n",
      "[SparseLR] Loaded model <SparseLR_13>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>l1_penalty</th>\n",
       "      <th>l2_penalty</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>Rec.</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.131148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.103093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.052288</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.098160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.048611</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.040609</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.077295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.037975</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.036765</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.068493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.036145</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.032967</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.033058</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.061069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.037736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.034483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.023810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lr  l1_penalty  l2_penalty     Prec.  Rec.        F1\n",
       "13  0.001000    0.000010    0.000100  0.078431   0.4  0.131148\n",
       "8   0.000100    0.010000    0.000001  0.057471   0.5  0.103093\n",
       "15  0.000010    0.000001    0.000100  0.052288   0.8  0.098160\n",
       "17  0.010000    0.000010    0.000001  0.048611   0.7  0.090909\n",
       "18  0.000010    0.010000    0.000001  0.044776   0.6  0.083333\n",
       "1   0.000100    0.000001    0.001000  0.040609   0.8  0.077295\n",
       "2   0.001000    0.000010    0.000010  0.037975   0.6  0.071429\n",
       "4   0.010000    0.000100    0.000010  0.040541   0.3  0.071429\n",
       "9   0.000001    0.000010    0.001000  0.036765   0.5  0.068493\n",
       "3   0.001000    0.000001    0.001000  0.036145   0.6  0.068182\n",
       "6   0.000001    0.001000    0.010000  0.035294   0.6  0.066667\n",
       "11  0.001000    0.000001    0.001000  0.032967   0.6  0.062500\n",
       "14  0.000100    0.010000    0.000010  0.033898   0.4  0.062500\n",
       "7   0.010000    0.000010    0.010000  0.033058   0.4  0.061069\n",
       "10  0.000100    0.010000    0.000100  0.025316   0.4  0.047619\n",
       "16  0.000001    0.010000    0.001000  0.025316   0.4  0.047619\n",
       "19  0.010000    0.000010    0.000100  0.022222   0.2  0.040000\n",
       "12  0.000100    0.000100    0.000010  0.020833   0.2  0.037736\n",
       "0   0.010000    0.001000    0.000100  0.020833   0.1  0.034483\n",
       "5   0.000001    0.001000    0.000010  0.013514   0.1  0.023810"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1701)\n",
    "searcher.fit(F_dev, L_gold_dev, n_epochs=50, rebalance=True, print_freq=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.4\n",
      "Neg. class accuracy: 0.795\n",
      "Precision            0.0784\n",
      "Recall               0.4\n",
      "F1                   0.131\n",
      "----------------------------------------\n",
      "TP: 4 | FP: 47 | TN: 182 | FN: 6\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, _, _, _ = disc_model.score(session, F_dev, L_gold_dev)"
   ]
  }
 ],
 "metadata": {
  "anaconda-butt": {},
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
